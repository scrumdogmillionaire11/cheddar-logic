---
phase: 01-cli-stabilization
plan: 05
type: tdd
wave: 3
depends_on: ["01-01", "01-02"]
files_modified:
  - src/cheddar_fpl_sage/analysis/decision_framework/transfer_advisor.py
  - src/cheddar_fpl_sage/analysis/decision_framework/chip_analyzer.py
  - tests/tests_new/test_manual_player_fallback.py
  - tests/tests_new/test_chip_window_edge_cases.py
autonomous: true

must_haves:
  truths:
    - "Manual players display with correct name (not 'Player 999999')"
    - "Manual players get fallback projections (not crash)"
    - "Chip window analysis with empty windows returns graceful fallback (not 'UNAVAILABLE')"
    - "Tests exist for manual player and chip window edge cases"
  artifacts:
    - path: "tests/tests_new/test_manual_player_fallback.py"
      provides: "Manual player edge case tests"
      contains: "def test_manual_player"
    - path: "tests/tests_new/test_chip_window_edge_cases.py"
      provides: "Chip window edge case tests"
      contains: "def test_empty_chip_windows"
  key_links:
    - from: "transfer_advisor.py"
      to: "constants.py"
      via: "is_manual_player"
      pattern: "is_manual_player"
    - from: "chip_analyzer.py"
      to: "models.py"
      via: "ChipRecommendation fallback"
      pattern: "ChipRecommendation"
---

<objective>
Fix known bugs (manual player display, chip window UNAVAILABLE) using TDD approach and add edge case tests.

Purpose: Address the two known bugs identified in CONCERNS.md with test-first methodology. Tests document expected behavior and prevent regression.

Output: Fixed bugs + comprehensive tests for manual player fallback and chip window edge cases.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-cli-stabilization/01-RESEARCH.md
@.planning/phases/01-cli-stabilization/01-01-SUMMARY.md
@.planning/phases/01-cli-stabilization/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: TDD - Manual player fallback tests and fix</name>
  <files>
    tests/tests_new/test_manual_player_fallback.py
    src/cheddar_fpl_sage/analysis/decision_framework/transfer_advisor.py
  </files>
  <action>
**RED Phase:** Write failing tests for manual player handling.

Create `tests/tests_new/test_manual_player_fallback.py`:
```python
"""
Tests for manual player fallback behavior.

Known bug: Manual players (ID >= 900000) display as "Player 999999 - $0.0m"
instead of their actual name. This test documents expected behavior.
"""
import pytest
from cheddar_fpl_sage.analysis.decision_framework import (
    TransferAdvisor, is_manual_player, MANUAL_PLAYER_ID_START,
    FALLBACK_PROJECTION_PTS
)


class TestIsManualPlayer:
    """Tests for manual player identification."""

    def test_fpl_player_not_manual(self):
        """Regular FPL player IDs are not manual."""
        assert is_manual_player(1) == False
        assert is_manual_player(100) == False
        assert is_manual_player(500000) == False

    def test_manual_player_id_range(self):
        """IDs >= MANUAL_PLAYER_ID_START are manual."""
        assert is_manual_player(MANUAL_PLAYER_ID_START) == True
        assert is_manual_player(999999) == True
        assert is_manual_player(900001) == True

    def test_boundary_id(self):
        """Boundary at MANUAL_PLAYER_ID_START."""
        assert is_manual_player(MANUAL_PLAYER_ID_START - 1) == False
        assert is_manual_player(MANUAL_PLAYER_ID_START) == True


class TestManualPlayerFallback:
    """Tests for manual player projection fallback."""

    @pytest.fixture
    def transfer_advisor(self):
        return TransferAdvisor()

    def test_fallback_projection_has_correct_name(self, transfer_advisor):
        """Manual player fallback uses actual name, not 'Player XXXXX'."""
        manual_player = {
            'player_id': 999999,
            'name': 'Collins',
            'position': 'DEF',
            'team': 'CRY'
        }
        fallback = transfer_advisor._create_fallback_projection(manual_player)

        # BUG: Currently shows "Player 999999" - should show "Collins"
        assert fallback['name'] == 'Collins'
        assert 'Player 999999' not in fallback['name']

    def test_fallback_projection_uses_constants(self, transfer_advisor):
        """Fallback projection uses centralized constant values."""
        manual_player = {
            'player_id': 900001,
            'name': 'Manual Test',
            'position': 'MID'
        }
        fallback = transfer_advisor._create_fallback_projection(manual_player)

        assert fallback['nextGW_pts'] == FALLBACK_PROJECTION_PTS

    def test_fallback_rejects_non_manual_player(self, transfer_advisor):
        """Calling fallback on regular player raises error."""
        regular_player = {'player_id': 100, 'name': 'Salah'}

        with pytest.raises(ValueError, match="manual player"):
            transfer_advisor._create_fallback_projection(regular_player)

    def test_fallback_handles_missing_fields(self, transfer_advisor):
        """Fallback works with minimal player data."""
        minimal = {'player_id': 999999}  # Only ID, no name
        fallback = transfer_advisor._create_fallback_projection(minimal)

        assert fallback['name'] == 'Manual Player'  # Default name
        assert fallback['position'] == 'DEF'  # Default position


class TestManualPlayerInSquad:
    """Tests for manual players in full squad context."""

    @pytest.fixture
    def advisor_with_squad(self):
        """Advisor with squad containing manual player."""
        return TransferAdvisor(), [
            {'player_id': 1, 'name': 'Salah', 'position': 'MID'},
            {'player_id': 999999, 'name': 'Collins', 'position': 'DEF', 'team': 'CRY'},
            {'player_id': 2, 'name': 'Haaland', 'position': 'FWD'},
        ]

    def test_manual_player_included_in_recommendations(self, advisor_with_squad):
        """Manual player appears correctly in squad analysis."""
        advisor, squad = advisor_with_squad
        projections = {
            1: {'nextGW_pts': 10.0, 'name': 'Salah'},
            2: {'nextGW_pts': 12.0, 'name': 'Haaland'},
            # 999999 NOT in projections - triggers fallback
        }

        # Process squad - manual player should get fallback, not crash
        processed = advisor._ensure_projections(squad, projections)
        collins = next(p for p in processed if p['player_id'] == 999999)

        assert collins['name'] == 'Collins'
        assert collins['nextGW_pts'] == FALLBACK_PROJECTION_PTS
```

Run tests (should FAIL - bug not fixed yet):
```bash
pytest tests/tests_new/test_manual_player_fallback.py -v
```

**GREEN Phase:** Fix the bug in TransferAdvisor.

In `transfer_advisor.py`, ensure:
1. `_create_fallback_projection()` uses player's actual name
2. Uses constants from `constants.py`
3. `_ensure_projections()` method handles missing projections

```python
def _create_fallback_projection(self, player: Dict) -> Dict:
    """
    Create conservative projection for manually added players.

    Uses the player's actual name and constants for projection values.
    """
    player_id = player.get('player_id', 0)
    if not is_manual_player(player_id):
        raise ValueError(f"Only call for manual players, got ID {player_id}")

    return {
        'player_id': player_id,
        'name': player.get('name', 'Manual Player'),  # Use actual name!
        'position': player.get('position', 'DEF'),
        'team': player.get('team', 'UNK'),
        'nextGW_pts': FALLBACK_PROJECTION_PTS,
        'next3GW_pts': FALLBACK_NEXT_3GW_PTS,
        'next5GW_pts': FALLBACK_NEXT_5GW_PTS,
        'is_manual': True,  # Flag for downstream processing
    }

def _ensure_projections(self, squad: List[Dict], projections: Dict[int, Any]) -> List[Dict]:
    """Ensure all squad members have projections, using fallback for manual players."""
    result = []
    for player in squad:
        player_id = player.get('player_id')
        if player_id in projections:
            # Merge player info with projection
            merged = {**player, **projections[player_id]}
            result.append(merged)
        elif is_manual_player(player_id):
            # Use fallback for manual players
            fallback = self._create_fallback_projection(player)
            result.append(fallback)
        else:
            logger.warning("No projection for player %s", player_id)
            result.append(player)
    return result
```

Run tests again (should PASS):
```bash
pytest tests/tests_new/test_manual_player_fallback.py -v
```

Commit:
```bash
git add tests/tests_new/test_manual_player_fallback.py
git commit -m "test(01-05): add failing tests for manual player fallback"

# After fix:
git add src/cheddar_fpl_sage/analysis/decision_framework/transfer_advisor.py
git commit -m "fix(01-05): manual players display correct name, use constants"
```
  </action>
  <verify>
```bash
pytest tests/tests_new/test_manual_player_fallback.py -v
# All tests should pass

# Verify manual player handling works end-to-end
python -c "
from cheddar_fpl_sage.analysis.decision_framework import TransferAdvisor, is_manual_player
advisor = TransferAdvisor()
manual = {'player_id': 999999, 'name': 'Collins', 'position': 'DEF'}
fb = advisor._create_fallback_projection(manual)
assert fb['name'] == 'Collins', f'Got {fb[\"name\"]}'
print('Manual player fix verified!')
"
```
  </verify>
  <done>Manual player fallback tests pass. Players display with correct name. Constants used for projection values.</done>
</task>

<task type="auto">
  <name>Task 2: TDD - Chip window edge case tests and fix</name>
  <files>
    tests/tests_new/test_chip_window_edge_cases.py
    src/cheddar_fpl_sage/analysis/decision_framework/chip_analyzer.py
  </files>
  <action>
**RED Phase:** Write failing tests for chip window edge cases.

Create `tests/tests_new/test_chip_window_edge_cases.py`:
```python
"""
Tests for chip window analysis edge cases.

Known bug: Chip window analysis returns "UNAVAILABLE (missing context)"
when chip_windows is empty or scoring fails.
"""
import pytest
from cheddar_fpl_sage.analysis.decision_framework import (
    ChipAnalyzer, ChipRecommendation, CHIP_NAMES
)


class TestEmptyChipWindows:
    """Tests for analysis with no defined chip windows."""

    @pytest.fixture
    def analyzer(self):
        return ChipAnalyzer(risk_posture="BALANCED")

    @pytest.fixture
    def minimal_context(self):
        """Minimal valid analysis context."""
        return {
            'squad_data': {'current_squad': []},
            'fixture_data': {},
            'projections': {},
            'chip_status': {chip: {'available': True} for chip in CHIP_NAMES},
            'current_gw': 20
        }

    def test_empty_windows_returns_recommendation(self, analyzer, minimal_context):
        """Empty chip windows should return valid recommendation, not 'UNAVAILABLE'."""
        minimal_context['chip_policy'] = {'chip_windows': []}

        result = analyzer.analyze_chip_decision(**minimal_context)

        # BUG: Currently returns None or "UNAVAILABLE"
        assert isinstance(result, ChipRecommendation)
        assert "UNAVAILABLE" not in result.reasoning

    def test_empty_windows_recommends_none(self, analyzer, minimal_context):
        """With no windows defined, recommend not using a chip."""
        minimal_context['chip_policy'] = {'chip_windows': []}

        result = analyzer.analyze_chip_decision(**minimal_context)

        assert result.chip == "None"
        assert result.use_this_gw == False

    def test_graceful_fallback_reasoning(self, analyzer, minimal_context):
        """Fallback provides useful reasoning, not error message."""
        minimal_context['chip_policy'] = {'chip_windows': []}

        result = analyzer.analyze_chip_decision(**minimal_context)

        assert "window" in result.reasoning.lower() or "defined" in result.reasoning.lower()
        assert result.confidence == "LOW"  # Low confidence without windows


class TestChipWindowScoring:
    """Tests for chip window scoring edge cases."""

    @pytest.fixture
    def analyzer(self):
        return ChipAnalyzer()

    def test_missing_fixture_data_graceful(self, analyzer):
        """Missing fixture data doesn't crash scoring."""
        context = {
            'squad_data': {'current_squad': [{'player_id': 1}]},
            'fixture_data': {},  # Empty
            'projections': {1: {'nextGW_pts': 5}},
            'chip_status': {chip: {'available': True} for chip in CHIP_NAMES},
            'current_gw': 20,
            'chip_policy': {
                'chip_windows': [{'start_gw': 20, 'end_gw': 25, 'chip': 'Bench Boost'}]
            }
        }

        # Should not crash
        result = analyzer.analyze_chip_decision(**context)
        assert isinstance(result, ChipRecommendation)

    def test_window_outside_current_gw(self, analyzer):
        """Windows not containing current GW handled correctly."""
        context = {
            'squad_data': {'current_squad': []},
            'fixture_data': {},
            'projections': {},
            'chip_status': {chip: {'available': True} for chip in CHIP_NAMES},
            'current_gw': 10,
            'chip_policy': {
                'chip_windows': [{'start_gw': 30, 'end_gw': 35, 'chip': 'Wildcard'}]
            }
        }

        result = analyzer.analyze_chip_decision(**context)

        # Current GW not in any window
        assert result.use_this_gw == False
        assert result.optimal_window_gw == 30  # Suggests future window


class TestChipAvailability:
    """Tests for chip status handling."""

    @pytest.fixture
    def analyzer(self):
        return ChipAnalyzer()

    def test_unavailable_chip_not_recommended(self, analyzer):
        """Already-used chip not recommended."""
        context = {
            'squad_data': {'current_squad': []},
            'fixture_data': {},
            'projections': {},
            'chip_status': {
                'Wildcard': {'available': False, 'played_gw': 5},
                'Free Hit': {'available': True},
                'Bench Boost': {'available': True},
                'Triple Captain': {'available': True}
            },
            'current_gw': 20,
            'chip_policy': {
                'chip_windows': [{'start_gw': 20, 'end_gw': 20, 'chip': 'Wildcard'}]
            }
        }

        result = analyzer.analyze_chip_decision(**context)

        # Can't recommend Wildcard - already played
        if result.chip == "Wildcard":
            pytest.fail("Recommended unavailable chip")

    def test_all_chips_used_returns_none(self, analyzer):
        """When all chips used, returns None chip."""
        context = {
            'squad_data': {'current_squad': []},
            'fixture_data': {},
            'projections': {},
            'chip_status': {chip: {'available': False, 'played_gw': i}
                          for i, chip in enumerate(CHIP_NAMES)},
            'current_gw': 20,
            'chip_policy': {'chip_windows': []}
        }

        result = analyzer.analyze_chip_decision(**context)

        assert result.chip == "None"
        assert "no chips available" in result.reasoning.lower() or \
               "all chips" in result.reasoning.lower()
```

Run tests (should FAIL):
```bash
pytest tests/tests_new/test_chip_window_edge_cases.py -v
```

**GREEN Phase:** Fix ChipAnalyzer to handle edge cases.

In `chip_analyzer.py`, ensure:
1. Empty windows returns valid ChipRecommendation (not None/"UNAVAILABLE")
2. Missing fixture data doesn't crash
3. Unavailable chips filtered out

```python
def analyze_chip_decision(
    self,
    squad_data: Dict[str, Any],
    fixture_data: Dict[str, Any],
    projections: Dict[int, Any],
    chip_status: Dict[str, Any],
    current_gw: int,
    chip_policy: Optional[Dict[str, Any]] = None
) -> ChipRecommendation:
    """
    Analyze which chip (if any) to play this gameweek.

    Always returns a valid ChipRecommendation, even for edge cases.
    """
    # Get available chips
    available_chips = [
        chip for chip in CHIP_NAMES
        if chip_status.get(chip, {}).get('available', True)
    ]

    if not available_chips:
        return ChipRecommendation(
            chip="None",
            use_this_gw=False,
            reasoning="No chips available - all have been used.",
            confidence="HIGH"
        )

    # Get chip windows
    windows = (chip_policy or {}).get('chip_windows', [])

    if not windows:
        return ChipRecommendation(
            chip="None",
            use_this_gw=False,
            reasoning="No chip windows defined. Consider defining optimal windows in config.",
            confidence="LOW"
        )

    # Find windows containing current GW
    current_windows = [
        w for w in windows
        if w.get('start_gw', 0) <= current_gw <= w.get('end_gw', 0)
        and w.get('chip') in available_chips
    ]

    if not current_windows:
        # Find next available window
        future_windows = [
            w for w in windows
            if w.get('start_gw', 0) > current_gw
            and w.get('chip') in available_chips
        ]
        if future_windows:
            next_window = min(future_windows, key=lambda w: w['start_gw'])
            return ChipRecommendation(
                chip="None",
                use_this_gw=False,
                optimal_window_gw=next_window['start_gw'],
                reasoning=f"Save chips. Next optimal window: GW{next_window['start_gw']} for {next_window['chip']}.",
                confidence="MEDIUM"
            )
        else:
            return ChipRecommendation(
                chip="None",
                use_this_gw=False,
                reasoning="No optimal chip windows for remaining gameweeks.",
                confidence="LOW"
            )

    # Score current windows and select best
    # [Existing scoring logic, wrapped in try/except for safety]
    try:
        best = self._score_and_select_chip(current_windows, squad_data, fixture_data, projections)
        return best
    except Exception as e:
        logger.warning("Chip scoring failed: %s, returning conservative recommendation", e)
        return ChipRecommendation(
            chip="None",
            use_this_gw=False,
            reasoning=f"Chip analysis incomplete. Consider manual review.",
            confidence="LOW"
        )
```

Run tests again (should PASS):
```bash
pytest tests/tests_new/test_chip_window_edge_cases.py -v
```

Commit:
```bash
git add tests/tests_new/test_chip_window_edge_cases.py
git commit -m "test(01-05): add failing tests for chip window edge cases"

# After fix:
git add src/cheddar_fpl_sage/analysis/decision_framework/chip_analyzer.py
git commit -m "fix(01-05): chip window returns graceful fallback, not UNAVAILABLE"
```
  </action>
  <verify>
```bash
pytest tests/tests_new/test_chip_window_edge_cases.py -v
# All tests should pass

# Verify no "UNAVAILABLE" in output
python -c "
from cheddar_fpl_sage.analysis.decision_framework import ChipAnalyzer, CHIP_NAMES
analyzer = ChipAnalyzer()
result = analyzer.analyze_chip_decision(
    squad_data={'current_squad': []},
    fixture_data={},
    projections={},
    chip_status={c: {'available': True} for c in CHIP_NAMES},
    current_gw=20,
    chip_policy={'chip_windows': []}
)
assert 'UNAVAILABLE' not in result.reasoning
print(f'Chip recommendation: {result.chip}, reason: {result.reasoning}')
print('Chip window fix verified!')
"
```
  </verify>
  <done>Chip window edge case tests pass. Empty windows return graceful fallback. No "UNAVAILABLE" in output.</done>
</task>

<task type="auto">
  <name>Task 3: Integration test for full analysis flow</name>
  <files>
    tests/tests_new/test_stabilization_integration.py
  </files>
  <action>
Create integration test that exercises the full decision flow with edge cases.

Create `tests/tests_new/test_stabilization_integration.py`:
```python
"""
Integration tests for Phase 1 CLI stabilization.

Verifies the complete analysis flow works with:
- Manual players in squad
- Empty chip windows
- Missing projections
- Config round-tripping
"""
import pytest
import json
import tempfile
from pathlib import Path

from cheddar_fpl_sage.analysis.decision_framework import (
    ChipAnalyzer, TransferAdvisor, CaptainSelector, OutputFormatter,
    TeamConfig, DecisionSummary,
    is_manual_player, CHIP_NAMES, FALLBACK_PROJECTION_PTS
)


class TestFullAnalysisFlow:
    """Tests for complete analysis with edge cases."""

    @pytest.fixture
    def squad_with_manual_player(self):
        """Squad containing manual player Collins."""
        return [
            {'player_id': 100, 'name': 'Onana', 'position': 'GKP', 'team': 'AVL'},
            {'player_id': 101, 'name': 'Saliba', 'position': 'DEF', 'team': 'ARS'},
            {'player_id': 999999, 'name': 'Collins', 'position': 'DEF', 'team': 'CRY'},
            {'player_id': 103, 'name': 'Gabriel', 'position': 'DEF', 'team': 'ARS'},
            {'player_id': 104, 'name': 'Saka', 'position': 'MID', 'team': 'ARS'},
            {'player_id': 105, 'name': 'Salah', 'position': 'MID', 'team': 'LIV'},
        ]

    @pytest.fixture
    def projections_without_manual(self):
        """Projections missing the manual player."""
        return {
            100: {'name': 'Onana', 'nextGW_pts': 4.0},
            101: {'name': 'Saliba', 'nextGW_pts': 5.5},
            # 999999 MISSING - manual player
            103: {'name': 'Gabriel', 'nextGW_pts': 5.0},
            104: {'name': 'Saka', 'nextGW_pts': 7.5},
            105: {'name': 'Salah', 'nextGW_pts': 9.0},
        }

    def test_manual_player_gets_fallback_in_analysis(
        self, squad_with_manual_player, projections_without_manual
    ):
        """Manual player handled correctly in full analysis."""
        advisor = TransferAdvisor()

        # Process squad with missing projection
        processed = advisor._ensure_projections(
            squad_with_manual_player,
            projections_without_manual
        )

        # Find Collins
        collins = next(p for p in processed if p['player_id'] == 999999)

        # Should have name and fallback projection
        assert collins['name'] == 'Collins'
        assert collins['nextGW_pts'] == FALLBACK_PROJECTION_PTS

    def test_empty_windows_produces_valid_summary(self):
        """Empty chip windows still produces complete summary."""
        analyzer = ChipAnalyzer()
        selector = CaptainSelector()
        formatter = OutputFormatter()

        chip_rec = analyzer.analyze_chip_decision(
            squad_data={'current_squad': []},
            fixture_data={},
            projections={},
            chip_status={c: {'available': True} for c in CHIP_NAMES},
            current_gw=20,
            chip_policy={'chip_windows': []}
        )

        # Should be valid recommendation
        assert chip_rec.chip == "None"
        assert "UNAVAILABLE" not in chip_rec.reasoning


class TestConfigRoundTrip:
    """Tests for config serialization stability."""

    def test_config_survives_save_reload(self, tmp_path):
        """Config identical after write/read cycle."""
        config_file = tmp_path / "test_config.json"

        original = TeamConfig(
            manager_id=12345,
            manager_name="Test Manager",
            risk_posture="CHASE",
            manual_free_transfers=2
        )

        # Write
        config_file.write_text(original.model_dump_json(indent=2))

        # Read
        reloaded = TeamConfig.model_validate_json(config_file.read_text())

        assert reloaded.manager_id == original.manager_id
        assert reloaded.manager_name == original.manager_name
        assert reloaded.risk_posture == original.risk_posture
        assert reloaded.manual_free_transfers == original.manual_free_transfers


class TestExceptionHandling:
    """Tests for proper exception handling."""

    def test_invalid_input_raises_specific_error(self):
        """Invalid inputs raise domain-specific errors, not generic Exception."""
        advisor = TransferAdvisor()

        # Calling fallback on non-manual player should raise ValueError
        with pytest.raises(ValueError, match="manual"):
            advisor._create_fallback_projection({'player_id': 100})

    def test_keyboard_interrupt_propagates(self):
        """KeyboardInterrupt not caught by error handlers."""
        import signal

        # This is a meta-test - verifies our exception handlers
        # don't accidentally catch KeyboardInterrupt
        # If this hangs, exception handling is broken

        def handler(signum, frame):
            raise KeyboardInterrupt()

        old_handler = signal.signal(signal.SIGALRM, handler)
        try:
            signal.alarm(1)  # Trigger in 1 second
            try:
                # Any code that might catch Exception
                advisor = TransferAdvisor()
                # If KeyboardInterrupt is caught, this won't raise
            except KeyboardInterrupt:
                pass  # Expected
            finally:
                signal.alarm(0)
        finally:
            signal.signal(signal.SIGALRM, old_handler)
```

Run all Phase 1 tests:
```bash
pytest tests/tests_new/test_manual_player_fallback.py \
       tests/tests_new/test_chip_window_edge_cases.py \
       tests/tests_new/test_config_validation.py \
       tests/tests_new/test_stabilization_integration.py -v
```
  </action>
  <verify>
```bash
# Run all new tests
pytest tests/tests_new/test_stabilization_integration.py -v

# Run full test suite
pytest tests/ -v --tb=short

# Verify no regressions
pytest tests/tests_new/ -v
```
  </verify>
  <done>Integration tests pass. Full analysis flow works with manual players and empty chip windows. Config round-trips cleanly.</done>
</task>

</tasks>

<verification>
All bug fixes and tests complete:
```bash
# All Phase 1 tests pass
pytest tests/tests_new/test_manual_player_fallback.py \
       tests/tests_new/test_chip_window_edge_cases.py \
       tests/tests_new/test_config_validation.py \
       tests/tests_new/test_stabilization_integration.py -v

# Full regression test
pytest tests/ -v

# Manual verification
python -c "
from cheddar_fpl_sage.analysis.decision_framework import *

# Manual player shows correct name
advisor = TransferAdvisor()
fb = advisor._create_fallback_projection({'player_id': 999999, 'name': 'Collins'})
assert fb['name'] == 'Collins'

# Chip window returns valid recommendation
analyzer = ChipAnalyzer()
rec = analyzer.analyze_chip_decision(
    squad_data={}, fixture_data={}, projections={},
    chip_status={c: {'available': True} for c in CHIP_NAMES},
    current_gw=20, chip_policy={'chip_windows': []}
)
assert 'UNAVAILABLE' not in rec.reasoning

print('All Phase 1 fixes verified!')
"
```
</verification>

<success_criteria>
- test_manual_player_fallback.py: 8+ tests pass
- test_chip_window_edge_cases.py: 8+ tests pass
- test_stabilization_integration.py: Integration tests pass
- Manual player "Collins" displays correctly (not "Player 999999")
- Chip window with empty config returns "None" recommendation with reasoning
- Config round-trips (write -> read -> same data)
- All existing tests still pass
</success_criteria>

<output>
After completion, create `.planning/phases/01-cli-stabilization/01-05-SUMMARY.md`
</output>
