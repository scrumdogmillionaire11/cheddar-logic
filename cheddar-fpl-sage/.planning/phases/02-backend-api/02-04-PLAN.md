---
phase: 02-backend-api
plan: 04
type: execute
wave: 3
depends_on: ["02-02"]
files_modified:
  - backend/middleware/__init__.py
  - backend/middleware/rate_limit.py
  - backend/services/cache_service.py
  - backend/main.py
  - backend/routers/analyze.py
  - tests/tests_new/test_rate_limiting.py
autonomous: true

user_setup:
  - service: redis
    why: "Rate limiting and response caching"
    env_vars:
      - name: FPL_SAGE_REDIS_URL
        source: "Local: redis://localhost:6379 or cloud Redis provider URL"
    dashboard_config: []

must_haves:
  truths:
    - "Requests exceeding 100/hour per IP return 429 status"
    - "Same team+gameweek analysis returns cached result within 5 minutes"
    - "Rate limit headers included in responses"
    - "Application works without Redis (graceful degradation)"
  artifacts:
    - path: "backend/middleware/rate_limit.py"
      provides: "Rate limiting middleware"
      contains: "RateLimitMiddleware"
    - path: "backend/services/cache_service.py"
      provides: "Redis caching service"
      contains: "get_cached_analysis"
  key_links:
    - from: "backend/middleware/rate_limit.py"
      to: "redis"
      via: "redis client connection"
      pattern: "redis\\.(get|set|incr)"
    - from: "backend/services/cache_service.py"
      to: "redis"
      via: "redis client for caching"
      pattern: "redis\\.(get|setex)"
---

<objective>
Implement Redis-based rate limiting and response caching.

Purpose: Protect the API from abuse with rate limiting (100 req/hr per IP) and improve performance by caching repeated analysis requests for the same team/gameweek.

Output:
- Rate limiting middleware (100 requests/hour per IP)
- Response caching for analysis results (5 minute TTL)
- Graceful degradation when Redis unavailable
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/02-backend-api/02-02-SUMMARY.md
@backend/main.py
@backend/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create rate limiting middleware</name>
  <files>
    backend/middleware/__init__.py
    backend/middleware/rate_limit.py
    backend/config.py
  </files>
  <action>
**Create backend/middleware/__init__.py:**
```python
"""Middleware components."""
from .rate_limit import RateLimitMiddleware

__all__ = ["RateLimitMiddleware"]
```

**Create backend/middleware/rate_limit.py:**

```python
"""
Rate limiting middleware using Redis.
Implements sliding window rate limiting with graceful degradation.
"""
import logging
import time
from typing import Optional, Tuple

from fastapi import Request, Response
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.responses import JSONResponse

logger = logging.getLogger(__name__)


class RateLimitMiddleware(BaseHTTPMiddleware):
    """
    Rate limiting middleware using Redis sliding window.

    - 100 requests per hour per IP address
    - Returns 429 Too Many Requests when exceeded
    - Gracefully degrades to no-limit when Redis unavailable
    - Adds X-RateLimit-* headers to responses
    """

    def __init__(self, app, redis_client=None, requests_per_hour: int = 100):
        super().__init__(app)
        self.redis = redis_client
        self.requests_per_hour = requests_per_hour
        self.window_seconds = 3600  # 1 hour

    def _get_client_ip(self, request: Request) -> str:
        """Extract client IP, respecting X-Forwarded-For."""
        forwarded = request.headers.get("X-Forwarded-For")
        if forwarded:
            # Take first IP in chain (original client)
            return forwarded.split(",")[0].strip()
        return request.client.host if request.client else "unknown"

    def _check_rate_limit(self, client_ip: str) -> Tuple[bool, int, int]:
        """
        Check if request is within rate limit.

        Returns: (allowed, remaining, reset_time)
        """
        if not self.redis:
            # No Redis = no rate limiting (graceful degradation)
            return True, self.requests_per_hour, 0

        key = f"rate_limit:{client_ip}"
        now = int(time.time())
        window_start = now - self.window_seconds

        try:
            # Use Redis pipeline for atomic operations
            pipe = self.redis.pipeline()

            # Remove old entries outside window
            pipe.zremrangebyscore(key, 0, window_start)

            # Count requests in current window
            pipe.zcard(key)

            # Add current request
            pipe.zadd(key, {str(now): now})

            # Set expiry on key
            pipe.expire(key, self.window_seconds)

            results = pipe.execute()
            request_count = results[1]

            remaining = max(0, self.requests_per_hour - request_count - 1)
            reset_time = now + self.window_seconds

            if request_count >= self.requests_per_hour:
                return False, 0, reset_time

            return True, remaining, reset_time

        except Exception as e:
            logger.warning(f"Redis rate limit check failed: {e}")
            # Graceful degradation - allow request
            return True, self.requests_per_hour, 0

    async def dispatch(self, request: Request, call_next):
        """Process request with rate limiting."""
        # Skip rate limiting for health checks and docs
        if request.url.path in ["/health", "/", "/docs", "/openapi.json", "/redoc"]:
            return await call_next(request)

        client_ip = self._get_client_ip(request)
        allowed, remaining, reset_time = self._check_rate_limit(client_ip)

        if not allowed:
            return JSONResponse(
                status_code=429,
                content={
                    "error": "Rate limit exceeded",
                    "detail": f"Maximum {self.requests_per_hour} requests per hour",
                    "code": "RATE_LIMITED",
                    "retry_after": reset_time - int(time.time()),
                },
                headers={
                    "X-RateLimit-Limit": str(self.requests_per_hour),
                    "X-RateLimit-Remaining": "0",
                    "X-RateLimit-Reset": str(reset_time),
                    "Retry-After": str(reset_time - int(time.time())),
                },
            )

        # Process request
        response = await call_next(request)

        # Add rate limit headers
        response.headers["X-RateLimit-Limit"] = str(self.requests_per_hour)
        response.headers["X-RateLimit-Remaining"] = str(remaining)
        if reset_time:
            response.headers["X-RateLimit-Reset"] = str(reset_time)

        return response
```

**Update backend/config.py** to add rate limit settings:

```python
"""
Configuration settings for FPL Sage API.
"""
from pydantic_settings import BaseSettings
from typing import Optional


class Settings(BaseSettings):
    """Application settings loaded from environment variables."""

    # Redis
    REDIS_URL: str = "redis://localhost:6379"

    # API
    DEBUG: bool = False
    API_V1_PREFIX: str = "/api/v1"

    # Rate limiting
    RATE_LIMIT_REQUESTS_PER_HOUR: int = 100
    RATE_LIMIT_ENABLED: bool = True

    # Caching
    CACHE_TTL_SECONDS: int = 300  # 5 minutes
    CACHE_ENABLED: bool = True

    class Config:
        env_prefix = "FPL_SAGE_"
        case_sensitive = False


settings = Settings()
```
  </action>
  <verify>
```bash
cd /Users/ajcolubiale/projects/cheddar-fpl-sage
python -m py_compile backend/middleware/__init__.py
python -m py_compile backend/middleware/rate_limit.py
python -m py_compile backend/config.py
```
  </verify>
  <done>
- RateLimitMiddleware created with sliding window algorithm
- Graceful degradation when Redis unavailable
- X-RateLimit-* headers added to responses
- Config updated with rate limit settings
  </done>
</task>

<task type="auto">
  <name>Task 2: Create cache service for analysis results</name>
  <files>
    backend/services/cache_service.py
    backend/services/__init__.py
    backend/routers/analyze.py
  </files>
  <action>
**Create backend/services/cache_service.py:**

```python
"""
Caching service for analysis results using Redis.
"""
import logging
import json
import hashlib
from typing import Optional, Dict, Any

logger = logging.getLogger(__name__)


class CacheService:
    """
    Caches analysis results in Redis.

    Cache key: fpl_sage:analysis:{team_id}:{gameweek}
    TTL: 5 minutes (configurable)
    """

    def __init__(self, redis_client=None, ttl_seconds: int = 300):
        self.redis = redis_client
        self.ttl = ttl_seconds

    def _make_key(self, team_id: int, gameweek: Optional[int]) -> str:
        """Generate cache key."""
        gw = gameweek or "current"
        return f"fpl_sage:analysis:{team_id}:{gw}"

    def get_cached_analysis(
        self, team_id: int, gameweek: Optional[int] = None
    ) -> Optional[Dict[str, Any]]:
        """
        Get cached analysis result if available.

        Returns None if not cached or Redis unavailable.
        """
        if not self.redis:
            return None

        key = self._make_key(team_id, gameweek)

        try:
            cached = self.redis.get(key)
            if cached:
                logger.info(f"Cache hit for {key}")
                return json.loads(cached)
            return None
        except Exception as e:
            logger.warning(f"Cache get failed: {e}")
            return None

    def cache_analysis(
        self,
        team_id: int,
        gameweek: Optional[int],
        results: Dict[str, Any],
    ) -> bool:
        """
        Cache analysis results.

        Returns True if cached successfully, False otherwise.
        """
        if not self.redis:
            return False

        key = self._make_key(team_id, gameweek)

        try:
            # Serialize results
            serialized = json.dumps(results, default=str)

            # Store with TTL
            self.redis.setex(key, self.ttl, serialized)
            logger.info(f"Cached analysis for {key}, TTL={self.ttl}s")
            return True
        except Exception as e:
            logger.warning(f"Cache set failed: {e}")
            return False

    def invalidate(self, team_id: int, gameweek: Optional[int] = None) -> bool:
        """Invalidate cached analysis."""
        if not self.redis:
            return False

        key = self._make_key(team_id, gameweek)

        try:
            self.redis.delete(key)
            return True
        except Exception as e:
            logger.warning(f"Cache invalidate failed: {e}")
            return False


# Singleton - will be initialized with Redis in main.py
cache_service = CacheService()
```

**Update backend/services/__init__.py:**
```python
"""Backend services."""
from .engine_service import engine_service, EngineService, AnalysisJob
from .cache_service import cache_service, CacheService

__all__ = [
    "engine_service",
    "EngineService",
    "AnalysisJob",
    "cache_service",
    "CacheService",
]
```

**Update backend/routers/analyze.py** to use caching:

Add at top after imports:
```python
from backend.services.cache_service import cache_service
```

Update the trigger_analysis function to check cache:

```python
@router.post(
    "",
    response_model=AnalyzeResponse,
    status_code=status.HTTP_202_ACCEPTED,
    responses={
        200: {"model": AnalysisStatus, "description": "Cached result returned"},
        400: {"model": ErrorResponse, "description": "Invalid request"},
        429: {"model": ErrorResponse, "description": "Rate limited"},
    },
)
async def trigger_analysis(
    request: AnalyzeRequest,
    background_tasks: BackgroundTasks,
):
    """
    Trigger a new FPL analysis for the given team.

    - **team_id**: FPL team ID (required, 1-20000000 range)
    - **gameweek**: Target gameweek (optional, defaults to current)

    Returns cached result immediately if available (within 5 minutes).
    Otherwise returns an analysis_id that can be used to poll for results.
    """
    # Validate team_id range
    if request.team_id < 1 or request.team_id > 20_000_000:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail={
                "error": "Invalid team_id",
                "detail": f"team_id must be between 1 and 20000000, got {request.team_id}",
                "code": "INVALID_TEAM_ID",
            },
        )

    # Validate gameweek if provided
    if request.gameweek is not None:
        if request.gameweek < 1 or request.gameweek > 38:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail={
                    "error": "Invalid gameweek",
                    "detail": f"gameweek must be between 1 and 38, got {request.gameweek}",
                    "code": "INVALID_GAMEWEEK",
                },
            )

    # Check cache first
    cached_result = cache_service.get_cached_analysis(request.team_id, request.gameweek)
    if cached_result:
        logger.info(f"Returning cached result for team {request.team_id}")
        return JSONResponse(
            status_code=status.HTTP_200_OK,
            content={
                "status": "completed",
                "results": cached_result,
                "cached": True,
            },
            headers={"X-Cache": "HIT"},
        )

    # Create analysis job
    job = engine_service.create_analysis(request.team_id, request.gameweek)
    logger.info(f"Created analysis job {job.analysis_id} for team {request.team_id}")

    # Schedule background task
    background_tasks.add_task(
        run_analysis_task,
        job.analysis_id,
        request.team_id,
        request.gameweek,
    )

    return AnalyzeResponse(
        analysis_id=job.analysis_id,
        status="queued",
        created_at=job.created_at,
    )


async def run_analysis_task(
    analysis_id: str,
    team_id: int,
    gameweek: Optional[int],
):
    """Background task to run the analysis and cache results."""
    try:
        results = await engine_service.run_analysis(analysis_id)

        # Cache successful results
        cache_service.cache_analysis(team_id, gameweek, results)

        logger.info(f"Analysis {analysis_id} completed and cached")
    except Exception as e:
        logger.exception(f"Analysis {analysis_id} failed: {e}")
```

Also add this import at top:
```python
from fastapi.responses import JSONResponse
```
  </action>
  <verify>
```bash
cd /Users/ajcolubiale/projects/cheddar-fpl-sage
python -m py_compile backend/services/cache_service.py
python -m py_compile backend/routers/analyze.py
```
  </verify>
  <done>
- CacheService created with get/set/invalidate methods
- Analysis results cached for 5 minutes
- Cached results returned immediately with X-Cache: HIT header
- Graceful degradation when Redis unavailable
  </done>
</task>

<task type="auto">
  <name>Task 3: Wire middleware and services in main app</name>
  <files>
    backend/main.py
    tests/tests_new/test_rate_limiting.py
  </files>
  <action>
**Update backend/main.py** to include rate limiting and initialize Redis:

```python
"""
FPL Sage API - Main Application
"""
from contextlib import asynccontextmanager
from datetime import datetime, timezone
import logging
from typing import Optional

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import redis

from backend.config import settings
from backend.routers import analyze_router
from backend.middleware import RateLimitMiddleware
from backend.services.cache_service import cache_service

logging.basicConfig(level=logging.INFO if not settings.DEBUG else logging.DEBUG)
logger = logging.getLogger(__name__)

# Global Redis client
redis_client: Optional[redis.Redis] = None


def get_redis_client() -> Optional[redis.Redis]:
    """Get Redis client with connection pooling."""
    global redis_client
    if redis_client is not None:
        return redis_client

    try:
        redis_client = redis.from_url(
            settings.REDIS_URL,
            decode_responses=True,
            socket_connect_timeout=2,
            socket_timeout=2,
        )
        # Test connection
        redis_client.ping()
        logger.info(f"Connected to Redis at {settings.REDIS_URL}")
        return redis_client
    except Exception as e:
        logger.warning(f"Redis connection failed: {e}. Running without cache/rate limiting.")
        return None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan context manager."""
    logger.info("FPL Sage API starting up...")

    # Initialize Redis connection
    client = get_redis_client()

    # Configure services with Redis
    if client:
        cache_service.redis = client
        cache_service.ttl = settings.CACHE_TTL_SECONDS

    yield

    # Cleanup
    if redis_client:
        redis_client.close()
    logger.info("FPL Sage API shutting down...")


app = FastAPI(
    title="FPL Sage API",
    description="AI-powered FPL decision engine API",
    version="1.0.0",
    lifespan=lifespan,
)

# CORS middleware (must be first)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Restrict in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Rate limiting middleware
if settings.RATE_LIMIT_ENABLED:
    app.add_middleware(
        RateLimitMiddleware,
        redis_client=get_redis_client(),
        requests_per_hour=settings.RATE_LIMIT_REQUESTS_PER_HOUR,
    )

# Include routers
app.include_router(analyze_router, prefix=settings.API_V1_PREFIX)


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    redis_status = "connected" if redis_client else "disconnected"
    return {
        "status": "healthy",
        "version": "1.0.0",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "redis": redis_status,
    }


@app.get("/")
async def root():
    """Root endpoint with API info."""
    return {
        "name": "FPL Sage API",
        "version": "1.0.0",
        "docs": "/docs",
        "health": "/health",
    }
```

**Create tests/tests_new/test_rate_limiting.py:**

```python
"""
Tests for rate limiting and caching.
"""
import pytest
from unittest.mock import MagicMock, patch
import sys
import os

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'backend'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

from backend.middleware.rate_limit import RateLimitMiddleware
from backend.services.cache_service import CacheService


class TestRateLimitMiddleware:
    """Tests for rate limiting."""

    def test_no_redis_allows_all_requests(self):
        """Without Redis, all requests are allowed."""
        middleware = RateLimitMiddleware(app=None, redis_client=None)
        allowed, remaining, reset = middleware._check_rate_limit("127.0.0.1")

        assert allowed is True
        assert remaining == 100  # Default limit

    def test_extracts_client_ip_from_headers(self):
        """X-Forwarded-For header is respected."""
        middleware = RateLimitMiddleware(app=None)

        class MockRequest:
            headers = {"X-Forwarded-For": "1.2.3.4, 5.6.7.8"}
            client = MagicMock(host="10.0.0.1")

        ip = middleware._get_client_ip(MockRequest())
        assert ip == "1.2.3.4"

    def test_falls_back_to_client_host(self):
        """Falls back to client.host when no X-Forwarded-For."""
        middleware = RateLimitMiddleware(app=None)

        class MockRequest:
            headers = {}
            client = MagicMock(host="192.168.1.1")

        ip = middleware._get_client_ip(MockRequest())
        assert ip == "192.168.1.1"

    def test_rate_limit_with_mock_redis(self):
        """Rate limiting works with Redis."""
        mock_redis = MagicMock()
        mock_pipe = MagicMock()
        mock_redis.pipeline.return_value = mock_pipe

        # Simulate 50 requests in window
        mock_pipe.execute.return_value = [None, 50, None, None]

        middleware = RateLimitMiddleware(
            app=None,
            redis_client=mock_redis,
            requests_per_hour=100,
        )

        allowed, remaining, reset = middleware._check_rate_limit("test_ip")

        assert allowed is True
        assert remaining == 49  # 100 - 50 - 1

    def test_rate_limit_exceeded(self):
        """Rate limit returns False when exceeded."""
        mock_redis = MagicMock()
        mock_pipe = MagicMock()
        mock_redis.pipeline.return_value = mock_pipe

        # Simulate 100 requests in window (at limit)
        mock_pipe.execute.return_value = [None, 100, None, None]

        middleware = RateLimitMiddleware(
            app=None,
            redis_client=mock_redis,
            requests_per_hour=100,
        )

        allowed, remaining, reset = middleware._check_rate_limit("test_ip")

        assert allowed is False
        assert remaining == 0


class TestCacheService:
    """Tests for caching service."""

    def test_no_redis_returns_none(self):
        """Without Redis, cache returns None."""
        cache = CacheService(redis_client=None)
        result = cache.get_cached_analysis(12345, 25)
        assert result is None

    def test_cache_miss_returns_none(self):
        """Cache miss returns None."""
        mock_redis = MagicMock()
        mock_redis.get.return_value = None

        cache = CacheService(redis_client=mock_redis)
        result = cache.get_cached_analysis(12345, 25)

        assert result is None
        mock_redis.get.assert_called_once()

    def test_cache_hit_returns_data(self):
        """Cache hit returns parsed data."""
        import json

        test_data = {"recommendations": ["test"]}
        mock_redis = MagicMock()
        mock_redis.get.return_value = json.dumps(test_data)

        cache = CacheService(redis_client=mock_redis)
        result = cache.get_cached_analysis(12345, 25)

        assert result == test_data

    def test_cache_set_with_ttl(self):
        """Cache stores with TTL."""
        mock_redis = MagicMock()

        cache = CacheService(redis_client=mock_redis, ttl_seconds=300)
        cache.cache_analysis(12345, 25, {"test": "data"})

        mock_redis.setex.assert_called_once()
        # Verify TTL is 300
        call_args = mock_redis.setex.call_args
        assert call_args[0][1] == 300  # TTL argument

    def test_cache_key_format(self):
        """Cache key includes team_id and gameweek."""
        cache = CacheService()

        key = cache._make_key(12345, 25)
        assert key == "fpl_sage:analysis:12345:25"

        key_no_gw = cache._make_key(12345, None)
        assert key_no_gw == "fpl_sage:analysis:12345:current"

    def test_invalidate_deletes_key(self):
        """Invalidate removes cache entry."""
        mock_redis = MagicMock()

        cache = CacheService(redis_client=mock_redis)
        cache.invalidate(12345, 25)

        mock_redis.delete.assert_called_once()
```
  </action>
  <verify>
```bash
cd /Users/ajcolubiale/projects/cheddar-fpl-sage

python -m py_compile backend/main.py

# Run rate limiting and caching tests
PYTHONPATH=src:backend python -m pytest tests/tests_new/test_rate_limiting.py -v
```
  </verify>
  <done>
- main.py initializes Redis connection on startup
- Rate limiting middleware wired with configurable limit
- Cache service initialized with Redis client
- Health endpoint shows Redis status
- 10+ tests for rate limiting and caching
  </done>
</task>

</tasks>

<verification>
After all tasks complete:

```bash
cd /Users/ajcolubiale/projects/cheddar-fpl-sage

# 1. Verify structure
ls -la backend/middleware/
ls -la backend/services/

# 2. Syntax check
python -m py_compile backend/main.py
python -m py_compile backend/middleware/rate_limit.py
python -m py_compile backend/services/cache_service.py

# 3. Run tests
PYTHONPATH=src:backend python -m pytest tests/tests_new/test_rate_limiting.py -v

# 4. Run full test suite
PYTHONPATH=src:backend python -m pytest tests/tests_new/test_api_endpoints.py tests/tests_new/test_rate_limiting.py -v

# 5. Test graceful degradation (no Redis)
PYTHONPATH=src:backend python -c "
from backend.services.cache_service import CacheService
cache = CacheService(redis_client=None)
result = cache.get_cached_analysis(123, 25)
print(f'No Redis, cache result: {result}')  # Should be None, no error
"
```
</verification>

<success_criteria>
- Rate limiting middleware returns 429 when limit exceeded
- X-RateLimit-* headers present in responses
- Cache service stores/retrieves analysis results
- Graceful degradation without Redis (no errors, just no caching)
- 10+ tests for rate limiting and caching pass
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-api/02-04-SUMMARY.md`
</output>
